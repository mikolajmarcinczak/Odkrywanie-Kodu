{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\John\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\John\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Only', 'poetry', 'or', 'madness', 'could', 'do', 'justice', 'to', 'the', 'noises'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gravity = TextBlob('Only poetry or madness could do justice to the noises.')\n",
    "gravity.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['madness', 'could', 'do', 'justice', 'to', 'the', 'noises', '.'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gravity.tokens[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['What', \"'s\", 'done', 'ca', \"n't\", 'be', 'undone', '.'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"What's done can't be undone.\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['What', 'is', 'done', 'can', 'not', 'be', 'undone', '.'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('What is done cannot be undone.').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Other', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " (\"'s\", 'POS'),\n",
       " ('views', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('troubles', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('contagious', 'JJ'),\n",
       " ('Do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('sabotage', 'VB'),\n",
       " ('yourself', 'PRP'),\n",
       " ('by', 'IN'),\n",
       " ('unwittingly', 'RB'),\n",
       " ('adopting', 'VBG'),\n",
       " ('negative', 'JJ'),\n",
       " ('unproductive', 'JJ'),\n",
       " ('attitudes', 'NNS'),\n",
       " ('through', 'IN'),\n",
       " ('your', 'PRP$'),\n",
       " ('associations', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('others', 'NNS')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wisdom = TextBlob(\"Other people's views and troubles can be contagious. Don't sabotage yourself by unwittingly adopting negative, unproductive attitudes through your associations with others.\")\n",
    "wisdom.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def countAdjectives(text):\n",
    "    return sum([1 if tag == 'JJ' else 0 for (word, tag) in text.tags])\n",
    "\n",
    "countAdjectives(wisdom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Wszyscy ludzie rodzą się wolni i równi pod względem swej godności i swych praw.\"),\n",
       " Sentence(\"Są oni obdarzeni rozumem i sumieniem i powinni postępować wobec innych w duchu braterstwa.\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pdfplumber\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "def str_between(s, start, end):\n",
    "    return re.sub('[\\n\\t\\r]', '', s[s.index(start)+len(start):s.index(end)]).strip()\n",
    "\n",
    "url = \"https://www.unesco.pl/fileadmin/user_upload/pdf/Powszechna_Deklaracja_Praw_Czlowieka.pdf\"\n",
    "req = requests.get(url)\n",
    "\n",
    "with pdfplumber.open(BytesIO(req.content)) as pdf:\n",
    "    text = pdf.pages[0].extract_text()\n",
    "    article1 = str_between(text, 'Artykuł 1', 'Artykuł 2')\n",
    "    \n",
    "tb_ar1 = TextBlob(article1)\n",
    "tb_ar1.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7335"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pride = open('pg42671.txt').read()\n",
    "prideblob = TextBlob(pride)\n",
    "countAdjectives(prideblob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.81 %'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adjsToWords(text):\n",
    "    return str(round(float(countAdjectives(text)/len(text.words))*100, 2)) + ' %'\n",
    "\n",
    "adjsToWords(prideblob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.688252023627214\n",
      "43.373347593582885\n",
      "31.09156193895871\n"
     ]
    }
   ],
   "source": [
    "def mean_length(sequence):\n",
    "    return sum(len(string) if string.strip() else 0 for string in sequence)/len(sequence)\n",
    "\n",
    "pride = open('pg42671.txt').readlines()\n",
    "iliad = open('pg16452.txt', encoding=\"utf8\").readlines()\n",
    "amores = open('pg22531.txt', encoding=\"utf8\").readlines()\n",
    "\n",
    "print(mean_length(pride))\n",
    "print(mean_length(iliad))\n",
    "print(mean_length(amores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.38990698129296"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean_length(pride) + mean_length(amores) ) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verseOrProse(text):\n",
    "    return 'verse' if mean_length(text) < 41.4 else 'prose'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prose'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unesco = open('ONZ1948.html').readlines()\n",
    "verseOrProse(unesco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.19375, subjectivity=0.6)\n",
      "Sentiment(polarity=-0.375, subjectivity=0.3)\n"
     ]
    }
   ],
   "source": [
    "print(TextBlob('Windows absolutely rocks. It never sucks. Start me up!').sentiment)\n",
    "print(TextBlob('Windows ME sucks rocks. Never start it up!').sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "print(TextBlob('If the composite of two thermodynamic systems constitutes an isolated system, then any energy exchange in any form between those two systems is bounded.').sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = open('english_words.txt')\n",
    "words = source.read().split()\n",
    "source.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGARAGAR\n",
      "BERBER\n",
      "BONBON\n",
      "BOOBOO\n",
      "BYEBYE\n",
      "CANCAN\n",
      "CHACHA\n",
      "COUSCOUS\n",
      "DODO\n",
      "GAGA\n",
      "HAHA\n",
      "HUSHHUSH\n",
      "ISIS\n",
      "KHOIKHOI\n",
      "LULU\n",
      "MAMA\n",
      "MAUMAU\n",
      "MURMUR\n",
      "PAPA\n",
      "PAWPAW\n",
      "PUTPUT\n",
      "SOSO\n",
      "TARTAR\n",
      "TESTES\n",
      "TOITOI\n",
      "TOMTOM\n",
      "TSETSE\n",
      "TUTU\n",
      "UHUH\n",
      "YOYO\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "def taut(_list):\n",
    "    sum=0\n",
    "    for word in _list:\n",
    "        if word[:(len(word)//2)] == word[(len(word)//2):]: #word[:len(word)/2]*2 == word\n",
    "            print(word)\n",
    "            sum +=1\n",
    "    print(sum)\n",
    "        \n",
    "taut(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\John\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\John\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from textblob import Word\n",
    "from textblob.wordnet import NOUN, VERB, ADJ, ADV\n",
    "generator = Word('generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generator']\n",
      "['generator']\n",
      "['generator', 'source', 'author']\n",
      "['generator']\n"
     ]
    }
   ],
   "source": [
    "syns = generator.synsets\n",
    "\n",
    "for set in syns:\n",
    "    print(set.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_w = Word('car')\n",
    "car1 = car_w.get_synsets(NOUN)[0]\n",
    "car1.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['motor_vehicle', 'automotive_vehicle']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car1.hypernyms()[0].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car1.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "river = Word('river').get_synsets(NOUN)[0]\n",
    "enigma = Word('enigma').get_synsets(NOUN)[0]\n",
    "river.path_similarity(enigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07142857142857142"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "river.path_similarity(car1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train_set = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "print('#n'.join(train_set.data[0].split('#n')[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "text_clf = text_clf.fit(train_set.data, train_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8169144981412639"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_set = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(test_set.data)\n",
    "np.mean(predicted == test_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.854089219330855"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "#Support Vector Machines\n",
    "text_clf_svm = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier())\n",
    "])\n",
    "\n",
    "_ = text_clf_svm.fit(train_set.data, train_set.target)\n",
    "\n",
    "predicted_svm = text_clf_svm.predict(test_set.data)\n",
    "np.mean(predicted_svm == test_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import NuSVR\n",
    "\n",
    "text_clf_nusvr = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf-nusvr', NuSVR(C=1., nu=0.1))\n",
    "])\n",
    "\n",
    "_ = text_clf_nusvr.fit(train_set.data, train_set.target)\n",
    "\n",
    "predicted_nusvr = text_clf_nusvr.predict(test_set.data)\n",
    "np.mean(predicted_nusvr == test_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-2, 1e-3)\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, params, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(train_set.data, train_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9129399133330441\n",
      "{'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904720110117279\n",
      "{'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "gs_clf_svm = GridSearchCV(text_clf_svm, params, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(train_set.data, train_set.target)\n",
    "\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
